"""
ãƒ©ã‚¤ãƒ–ãƒ©ãƒªAPI - ä¸Šä½ã‚·ã‚¹ãƒ†ãƒ çµ±åˆå‘ã‘ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import asyncio
import time
from typing import List, Dict, Any, Optional, Union
from pathlib import Path

from .main import AIAnalysisEngine as _InternalEngine
from .config.library_config import AnalysisConfig
from .models.result import AnalysisResult, Hypothesis, AnalysisMetrics
from .exceptions import (
    AIAnalysisError,
    ConfigurationError,
    ValidationError,
    AnalysisError,
    TimeoutError,
    InitializationError
)


class AIAnalysisEngine:
    """
    AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒª - ä¸Šä½ã‚·ã‚¹ãƒ†ãƒ çµ±åˆå‘ã‘

    ã‚·ãƒ³ãƒ—ãƒ«ã§ä½¿ã„ã‚„ã™ã„APIã‚’æä¾›ã—ã€è¤‡é›‘ãªå†…éƒ¨å‡¦ç†ã‚’éš è”½
    """

    def __init__(self, config: Optional[AnalysisConfig] = None):
        """
        ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–

        Args:
            config: åˆ†æè¨­å®šï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ï¼‰
        """
        self.config = config or AnalysisConfig.from_env()
        self._internal_engine = None
        self._rag_initialized = False
        self._algorithm_specs = []
        self._algorithm_codes = []
        self._evaluation_specs = []
        self._evaluation_codes = []

        # è¨­å®šæ¤œè¨¼
        try:
            self.config.validate()
        except ValueError as e:
            raise ConfigurationError(f"Invalid configuration: {e}")

    def initialize(
        self,
        algorithm_specs: List[str],
        algorithm_codes: List[str],
        evaluation_specs: List[str],
        evaluation_codes: List[str]
    ) -> bool:
        """
        ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã¨RAGãƒ™ã‚¯ãƒˆãƒ«åŒ–

        Args:
            algorithm_specs: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä»•æ§˜Markdownãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            algorithm_codes: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            evaluation_specs: è©•ä¾¡ä»•æ§˜Markdownãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            evaluation_codes: è©•ä¾¡ç’°å¢ƒã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

        Returns:
            bool: åˆæœŸåŒ–æˆåŠŸã®å ´åˆTrue

        Raises:
            InitializationError: åˆæœŸåŒ–å¤±æ•—æ™‚
            ValidationError: å…¥åŠ›æ¤œè¨¼ã‚¨ãƒ©ãƒ¼
        """
        # å…¥åŠ›æ¤œè¨¼
        self._validate_initialization_inputs(
            algorithm_specs, algorithm_codes, evaluation_specs, evaluation_codes
        )

        try:
            # å†…éƒ¨ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
            if self._internal_engine is None:
                self._internal_engine = _InternalEngine()

            if not self._internal_engine.initialize():
                raise InitializationError("Failed to initialize internal engine")

            # RAGãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¿å­˜
            self._algorithm_specs = algorithm_specs
            self._algorithm_codes = algorithm_codes
            self._evaluation_specs = evaluation_specs
            self._evaluation_codes = evaluation_codes

            # RAGãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®å®Ÿè¡Œ
            self._initialize_rag_vector_stores()
            self._rag_initialized = True

            return True

        except Exception as e:
            raise InitializationError(f"Engine initialization failed: {e}")

    def is_initialized(self) -> bool:
        """
        åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèª

        Returns:
            bool: åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆTrue
        """
        return (self._internal_engine is not None and
                self._rag_initialized and
                bool(self._algorithm_specs) and
                bool(self._algorithm_codes) and
                bool(self._evaluation_specs) and
                bool(self._evaluation_codes))

    def analyze(
        self,
        algorithm_outputs: List[str],
        core_outputs: List[str],
        expected_results: List[str],
        output_dir: str,
        dataset_ids: List[str],
        timeout: Optional[int] = None
    ) -> List[AnalysisResult]:
        """
        è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€æ‹¬åˆ†æã‚’å®Ÿè¡Œ

        Args:
            algorithm_outputs: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            core_outputs: ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            expected_results: æœŸå¾…ã•ã‚Œã‚‹çµæœã®è‡ªç„¶è¨€èªè¨˜è¿°ã®ãƒªã‚¹ãƒˆ
            output_dir: çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            dataset_ids: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®ãƒªã‚¹ãƒˆ
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ã€Noneã®å ´åˆã¯è¨­å®šå€¤ã‚’ä½¿ç”¨ï¼‰

        Returns:
            List[AnalysisResult]: åˆ†æçµæœã®ãƒªã‚¹ãƒˆ

        Raises:
            ValidationError: å…¥åŠ›æ¤œè¨¼ã‚¨ãƒ©ãƒ¼
            AnalysisError: åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼
            TimeoutError: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        """
        if not self.is_initialized():
            raise InitializationError("Engine not initialized. Call initialize() first.")

        # å…¥åŠ›æ¤œè¨¼
        self._validate_analysis_inputs(
            algorithm_outputs, core_outputs, expected_results, output_dir, dataset_ids
        )

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        actual_timeout = timeout or self.config.timeout
        results = []

        try:
            # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†
            for i, (algo_csv, core_csv, expected, dataset_id) in enumerate(zip(
                algorithm_outputs, core_outputs, expected_results, dataset_ids
            )):
                try:
                    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {i+1}/{len(dataset_ids)} ã‚’å‡¦ç†ä¸­: {dataset_id}")

                    # å†…éƒ¨ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
                    state = self._internal_engine.create_analysis_request(
                        algorithm_outputs=[algo_csv],
                        core_outputs=[core_csv],
                        algorithm_specs=self._algorithm_specs,
                        evaluation_specs=self._evaluation_specs,
                        expected_results=[expected],
                        algorithm_codes=self._algorithm_codes,
                        evaluation_codes=self._evaluation_codes,
                        dataset_ids=[dataset_id],
                        output_dir=output_dir
                    )

                    # åˆ†æå®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                    start_time = time.time()
                    result = self._run_with_timeout(
                        self._internal_engine.run_analysis(state),
                        actual_timeout
                    )
                    execution_time = time.time() - start_time

                    # çµæœã®å¤‰æ›
                    analysis_result = self._convert_to_library_result(
                        result, dataset_id, execution_time
                    )
                    results.append(analysis_result)

                    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ã®åˆ†æå®Œäº†")

                except Exception as e:
                    print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ã®åˆ†æå¤±æ•—: {e}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶™ç¶š
                    error_result = AnalysisResult.error_result(
                        dataset_id=dataset_id,
                        error=str(e),
                        error_details={"batch_index": i, "traceback": str(e)}
                    )
                    results.append(error_result)

            print(f"ğŸ“Š ä¸€æ‹¬åˆ†æå®Œäº†: {len(results)}/{len(dataset_ids)} ä»¶å‡¦ç†")
            return results

        except asyncio.TimeoutError:
            raise TimeoutError(f"Batch analysis timed out after {actual_timeout} seconds")
        except Exception as e:
            raise AnalysisError(f"Analysis failed: {e}")

    def _initialize_rag_vector_stores(self) -> None:
        """
        RAGãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–

        åˆæœŸåŒ–æ™‚ã«æä¾›ã•ã‚ŒãŸä»•æ§˜æ›¸ã¨ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        """
        try:
            from .tools.rag_tool import RAGTool

            print("ğŸ” RAGãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’é–‹å§‹...")

            # RAGãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–
            rag_tool = RAGTool()

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æº–å‚™
            documents = {
                "algorithm_specs": self._algorithm_specs,
                "algorithm_codes": self._algorithm_codes,
                "evaluation_specs": self._evaluation_specs,
                "evaluation_codes": self._evaluation_codes
            }

            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
            if rag_tool.initialize_vector_stores(documents):
                print("âœ… RAGãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†")
            else:
                print("âš ï¸ RAGãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§è­¦å‘ŠãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™")

        except Exception as e:
            print(f"âš ï¸ RAGãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™: {e}")

    def _validate_initialization_inputs(
        self,
        algorithm_specs: List[str],
        algorithm_codes: List[str],
        evaluation_specs: List[str],
        evaluation_codes: List[str]
    ) -> None:
        """
        åˆæœŸåŒ–å…¥åŠ›ã®æ¤œè¨¼

        Args:
            algorithm_specs: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä»•æ§˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            algorithm_codes: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            evaluation_specs: è©•ä¾¡ä»•æ§˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            evaluation_codes: è©•ä¾¡ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

        Raises:
            ValidationError: æ¤œè¨¼ã‚¨ãƒ©ãƒ¼
        """
        # å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
        if not algorithm_specs:
            raise ValidationError("algorithm_specs cannot be empty")
        if not algorithm_codes:
            raise ValidationError("algorithm_codes cannot be empty")
        if not evaluation_specs:
            raise ValidationError("evaluation_specs cannot be empty")
        if not evaluation_codes:
            raise ValidationError("evaluation_codes cannot be empty")

        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        all_files = algorithm_specs + algorithm_codes + evaluation_specs + evaluation_codes
        for file_path in all_files:
            if not Path(file_path).exists():
                raise ValidationError(f"File does not exist: {file_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        for spec_file in algorithm_specs + evaluation_specs:
            if not spec_file.lower().endswith(('.md', '.txt', '.markdown')):
                raise ValidationError(f"Specification file must be .md, .txt, or .markdown: {spec_file}")

        print(f"âœ… åˆæœŸåŒ–å…¥åŠ›æ¤œè¨¼å®Œäº†: {len(all_files)} ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªæ¸ˆã¿")

    def _validate_analysis_inputs(
        self,
        algorithm_outputs: List[str],
        core_outputs: List[str],
        expected_results: List[str],
        output_dir: str,
        dataset_ids: List[str]
    ) -> None:
        """
        åˆ†æå…¥åŠ›ã®æ¤œè¨¼

        Args:
            algorithm_outputs: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            core_outputs: ã‚³ã‚¢å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            expected_results: æœŸå¾…çµæœã®ãƒªã‚¹ãƒˆ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            dataset_ids: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®ãƒªã‚¹ãƒˆ

        Raises:
            ValidationError: æ¤œè¨¼ã‚¨ãƒ©ãƒ¼
        """
        # ãƒªã‚¹ãƒˆé•·ã®ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        lengths = [
            len(algorithm_outputs),
            len(core_outputs),
            len(expected_results),
            len(dataset_ids)
        ]

        if len(set(lengths)) != 1:
            raise ValidationError(
                f"All input lists must have the same length. "
                f"Got lengths: algorithm_outputs={len(algorithm_outputs)}, "
                f"core_outputs={len(core_outputs)}, "
                f"expected_results={len(expected_results)}, "
                f"dataset_ids={len(dataset_ids)}"
            )

        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        for file_path in algorithm_outputs + core_outputs:
            if not Path(file_path).exists():
                raise ValidationError(f"File does not exist: {file_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        for csv_file in algorithm_outputs + core_outputs:
            if not csv_file.lower().endswith('.csv'):
                raise ValidationError(f"Output file must be .csv: {csv_file}")

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒã‚§ãƒƒã‚¯
        if not output_dir.strip():
            raise ValidationError("output_dir cannot be empty")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDãƒã‚§ãƒƒã‚¯
        for dataset_id in dataset_ids:
            if not dataset_id.strip():
                raise ValidationError("dataset_ids cannot contain empty strings")

        # æœŸå¾…çµæœãƒã‚§ãƒƒã‚¯
        for expected in expected_results:
            if not expected.strip():
                raise ValidationError("expected_results cannot contain empty strings")

        print(f"âœ… åˆ†æå…¥åŠ›æ¤œè¨¼å®Œäº†: {len(algorithm_outputs)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèª")

    async def analyze_async(
        self,
        algorithm_outputs: List[str],
        core_outputs: List[str],
        expected_results: List[str],
        output_dir: str,
        dataset_ids: List[str],
        timeout: Optional[int] = None
    ) -> List[AnalysisResult]:
        """
        è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€æ‹¬åˆ†æã‚’éåŒæœŸå®Ÿè¡Œ

        Args:
            algorithm_outputs: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            core_outputs: ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            expected_results: æœŸå¾…ã•ã‚Œã‚‹çµæœã®è‡ªç„¶è¨€èªè¨˜è¿°ã®ãƒªã‚¹ãƒˆ
            output_dir: çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            dataset_ids: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®ãƒªã‚¹ãƒˆ
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ã€Noneã®å ´åˆã¯è¨­å®šå€¤ã‚’ä½¿ç”¨ï¼‰

        Returns:
            List[AnalysisResult]: åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
        """
        # ç¾åœ¨ã®å®Ÿè£…ã¯åŒæœŸå‡¦ç†ãªã®ã§ã€éåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ã‚’æä¾›
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self.analyze,
            algorithm_outputs,
            core_outputs,
            expected_results,
            output_dir,
            dataset_ids,
            timeout
        )


    def get_status(self) -> Dict[str, Any]:
        """
        ã‚¨ãƒ³ã‚¸ãƒ³ã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—

        Returns:
            Dict[str, Any]: çŠ¶æ…‹æƒ…å ±
        """
        if not self._internal_engine:
            return {
                "initialized": False,
                "rag_initialized": False,
                "config_valid": self.config.validate() if self.config else False,
                "version": "1.0.0"
            }

        internal_status = self._internal_engine.get_status()
        return {
            "initialized": self.is_initialized(),
            "rag_initialized": self._rag_initialized,
            "config_valid": self.config.validate() if self.config else False,
            "internal_engine_ready": internal_status.get("initialized", False),
            "documents_loaded": {
                "algorithm_specs": len(self._algorithm_specs),
                "algorithm_codes": len(self._algorithm_codes),
                "evaluation_specs": len(self._evaluation_specs),
                "evaluation_codes": len(self._evaluation_codes)
            },
            "version": "1.0.0",
            "config": {
                "model": self.config.model,
                "timeout": self.config.timeout,
                "output_dir": self.config.output_dir
            }
        }

    def shutdown(self) -> None:
        """
        ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

        å†…éƒ¨ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è¡Œã†
        """
        if self._internal_engine:
            # å†…éƒ¨ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            pass

        # RAGé–¢é€£ã®çŠ¶æ…‹ã‚¯ãƒªã‚¢
        self._rag_initialized = False
        self._algorithm_specs = []
        self._algorithm_codes = []
        self._evaluation_specs = []
        self._evaluation_codes = []

        self._internal_engine = None


    def _run_with_timeout(self, func_result, timeout: int):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§é–¢æ•°ã‚’å®Ÿè¡Œ"""
        import threading
        import queue

        result_queue = queue.Queue()

        def run_func():
            try:
                result_queue.put(func_result)
            except Exception as e:
                result_queue.put(e)

        thread = threading.Thread(target=run_func)
        thread.daemon = True
        thread.start()
        thread.join(timeout)

        if thread.is_alive():
            raise TimeoutError(f"Analysis timed out after {timeout} seconds")

        result = result_queue.get()
        if isinstance(result, Exception):
            raise result

        return result

    def _convert_to_library_result(
        self,
        internal_result: Dict[str, Any],
        dataset_id: str,
        execution_time: float
    ) -> AnalysisResult:
        """å†…éƒ¨çµæœã‚’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªçµæœã«å¤‰æ›"""
        if "error" in internal_result:
            return AnalysisResult.error_result(
                dataset_id=dataset_id,
                error=internal_result["error"],
                error_details=internal_result
            )

        # ä»®èª¬ã®æŠ½å‡º
        hypotheses = []
        if "datasets" in internal_result and internal_result["datasets"]:
            dataset = internal_result["datasets"][0]

            # ä»®èª¬ã®æŠ½å‡ºï¼ˆå†…éƒ¨å½¢å¼ã‹ã‚‰å¤‰æ›ï¼‰
            if hasattr(dataset, 'hypotheses') and dataset.hypotheses:
                for hyp in dataset.hypotheses:
                    if isinstance(hyp, dict):
                        hypotheses.append(Hypothesis(
                            text=hyp.get('hypothesis', ''),
                            confidence=hyp.get('confidence', 0.5),
                            evidence=hyp.get('evidence', []),
                            category=hyp.get('category', 'general')
                        ))

        # ãƒ¬ãƒãƒ¼ãƒˆã®å–å¾—
        report = None
        report_path = None
        if "datasets" in internal_result and internal_result["datasets"]:
            dataset = internal_result["datasets"][0]
            if hasattr(dataset, 'report_content') and dataset.report_content:
                report = dataset.report_content
                # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ¨å®š
                report_path = f"{self.config.output_dir}/results/reports/{dataset_id}_report.md"

        # ãƒ—ãƒ­ãƒƒãƒˆã®å–å¾—
        plots = []
        if "datasets" in internal_result and internal_result["datasets"]:
            # ãƒ—ãƒ­ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            import glob
            plot_pattern = f"{self.config.output_dir}/results/reports/plots/{dataset_id}/*.png"
            plots = glob.glob(plot_pattern)

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä½œæˆ
        metrics = AnalysisMetrics(
            execution_time=execution_time,
            data_points_processed=len(internal_result.get("datasets", [])),
            hypotheses_generated=len(hypotheses),
            plots_generated=len(plots)
        )

        return AnalysisResult.success_result(
            dataset_id=dataset_id,
            report=report,
            hypotheses=hypotheses,
            plots=plots,
            metrics=metrics,
            report_path=report_path,
            summary=f"Analysis completed for {dataset_id}"
        )
